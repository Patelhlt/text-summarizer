{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- xamine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give path to the data folder\n",
    "data_folder = 'data/'\n",
    "\n",
    "# This portion will load stopwords_en.txt\n",
    "with open('stopwords_en.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on basic text pre-processing\n",
    "\n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program run\n"
     ]
    }
   ],
   "source": [
    "# This is the regular expression for tokenization as per given\n",
    "token_pattern = re.compile(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "\n",
    "# This function will preprocess a single job description\n",
    "def preprocess_description(text):\n",
    "    tokens = token_pattern.findall(text)\n",
    "    tokens = [token.lower() for token in tokens if len(token) >= 2]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "# Extract descriptions and preprocess them\n",
    "documents = []\n",
    "all_tokens = []\n",
    "\n",
    "for category in os.listdir(data_folder):\n",
    "    category_path = os.path.join(data_folder, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        for job_file in os.listdir(category_path):\n",
    "            job_path = os.path.join(category_path, job_file)\n",
    "            with open(job_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                # Assume the description starts after a certain line (e.g., third line)\n",
    "                description = '\\n'.join(text.split('\\n')[2:])\n",
    "                tokens = preprocess_description(description)\n",
    "                documents.append((job_file, tokens))\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "# This will count term frequencies\n",
    "term_freq = Counter(all_tokens)\n",
    "\n",
    "# This will remove words wich appears only once\n",
    "documents = [(job_file, [token for token in tokens if term_freq[token] > 1]) for job_file, tokens in documents]\n",
    "\n",
    "# This will calculate document frequencies\n",
    "doc_freq = Counter()\n",
    "for job_file, tokens in documents:\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        doc_freq[token] += 1\n",
    "\n",
    "# This will extract top 50 mostfrequent words\n",
    "top_50_words = set(\n",
    "    [word for word, freq in doc_freq.most_common(50)]\n",
    ")\n",
    "\n",
    "# This will remove the extracted top 50 most frequent words\n",
    "documents = [\n",
    "    (job_file, [token for token in tokens if token not in top_50_words])\n",
    "    for job_file, tokens in documents\n",
    "]\n",
    "\n",
    "# This will build vocabulary\n",
    "vocab = sorted({token for job_file, tokens in documents for token in tokens})\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(\"Program run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5283\n"
     ]
    }
   ],
   "source": [
    "# This will save vocabulary\n",
    "with open('vocab.txt', 'w', encoding='utf-8') as file:\n",
    "    for word, idx in vocab_dict.items():\n",
    "        file.write(f\"{word}:{idx}\\n\")\n",
    "\n",
    "# This will save preprocessed job advertisements requirement for task 2_3\n",
    "with open('preprocessed_jobs.txt', 'w', encoding='utf-8') as file:\n",
    "    for job_file, tokens in documents:\n",
    "        file.write(f\"#{job_file},{' '.join(tokens)}\\n\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentence_scores, top_n=5):\n",
    "    sorted_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    summary = ' '.join([sentence for sentence, score in sorted_sentences[:top_n]])\n",
    "    return summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
